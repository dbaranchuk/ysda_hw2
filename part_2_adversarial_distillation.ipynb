{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882da312",
   "metadata": {},
   "source": [
    "# ДЗ №2 | Часть 2 | Adversarial Diffusion Distillation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U diffusers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18dffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline,UNet2DConditionModel, DDIMScheduler\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_num_threads(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------\n",
    "# Visualization utils\n",
    "#---------------------\n",
    "\n",
    "def visualize_images(images):\n",
    "    assert len(images) == 4\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=-0.01, hspace=-0.01)\n",
    "\n",
    "\n",
    "#--------------\n",
    "# Tensor utils\n",
    "#--------------\n",
    "\n",
    "def extract_into_tensor(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "#---------------\n",
    "# Dataset utils\n",
    "#---------------\n",
    "\n",
    "class TextImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, subset_name=\"train2014_5k\", transform=None, max_cnt=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            root_dir (string): Директория с картинками\n",
    "            transform (callable, optional): преобразования, применимые к картинкам\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.extensions = (\n",
    "            \".jpg\",\n",
    "            \".jpeg\",\n",
    "            \".png\",\n",
    "            \".ppm\",\n",
    "            \".bmp\",\n",
    "            \".pgm\",\n",
    "            \".tif\",\n",
    "            \".tiff\",\n",
    "            \".webp\",\n",
    "        )\n",
    "        sample_dir = os.path.join(root_dir, subset_name)\n",
    "\n",
    "        # Собираем пути до картинок\n",
    "        self.samples = sorted(\n",
    "            [\n",
    "                os.path.join(sample_dir, fname)\n",
    "                for fname in os.listdir(sample_dir)\n",
    "                if fname[-4:] in self.extensions\n",
    "            ],\n",
    "            key=lambda x: x.split(\"/\")[-1].split(\".\")[0],\n",
    "        )\n",
    "        self.samples = (\n",
    "            self.samples if max_cnt is None else self.samples[:max_cnt]\n",
    "        )  # \n",
    "\n",
    "        # Собираем промпты\n",
    "        self.captions = {}\n",
    "        with open(\n",
    "            os.path.join(root_dir, f\"{subset_name}.csv\"), newline=\"\\n\"\n",
    "        ) as csvfile:\n",
    "            spamreader = csv.reader(csvfile, delimiter=\",\")\n",
    "            for i, row in enumerate(spamreader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                self.captions[row[1]] = row[2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample_path = self.samples[idx]\n",
    "        sample = Image.open(sample_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return {\n",
    "            \"image\": sample,\n",
    "            \"text\": self.captions[os.path.basename(sample_path)],\n",
    "            \"idxs\": idx, }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f5a5e",
   "metadata": {},
   "source": [
    "# Модель учителя (SD1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f5200",
   "metadata": {},
   "source": [
    "Для начала загрузим модель [StableDiffusion 1.5](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5) и сгенерируем ей картинки за 50 шагов.\n",
    "\n",
    "**Важно:** для экономии памяти, загружаем все компоненты модели в FP16. Не забываем положить модель на GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f42d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = <YOUR CODE HERE>\n",
    "\n",
    "# Проверяем, что все компоненты модели в FP16 и на cuda\n",
    "assert pipe.unet.dtype == torch.float16 and pipe.unet.device.type == 'cuda'\n",
    "assert pipe.vae.dtype == torch.float16 and pipe.vae.device.type == 'cuda'\n",
    "assert pipe.text_encoder.dtype == torch.float16 and pipe.text_encoder.device.type == 'cuda'\n",
    "\n",
    "# Заменяем дефолтный сэмплер на DDIM\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "pipe.scheduler.timesteps = pipe.scheduler.timesteps.cuda()\n",
    "pipe.scheduler.alphas_cumprod = pipe.scheduler.alphas_cumprod.cuda()\n",
    "\n",
    "# Отдельно извлечем модель учителя, которую потом будем дистиллировать\n",
    "teacher_unet = pipe.unet\n",
    "teacher_unet.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdded0",
   "metadata": {},
   "source": [
    "##  Создаем датасет\n",
    "\n",
    "Мы будем дообучать модель на небольшой обучающей выборке из 10000 пар текст-картинка сгенерированные моделью [FLUX](https://huggingface.co/black-forest-labs/FLUX.1-dev).\n",
    "\n",
    "Данные можно загрузить с помощью команд в ячейке ниже. В текущей директории ./ должны появиться:\n",
    "* Папка flux_data с 10000 картинками\n",
    "* Файл flux_data.csv с 10000 промптами\n",
    "\n",
    "Данные парсятся корректным образом в уже реализованном классе **TextImageDataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f24013",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.yandexcloud.net/yandex-research/flux_data_10k.tar.gz\n",
    "!tar -xzf flux_data_10k.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c92388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(512),\n",
    "        transforms.CenterCrop(512),\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: 2 * x - 1,\n",
    "    ]\n",
    ")\n",
    "dataset = TextImageDataset(\".\",\n",
    "    subset_name=\"flux_data\",\n",
    "    transform=transform,\n",
    "    max_cnt=5000 # Для дебага лучше взять 1000 или меньше\n",
    ")\n",
    "\n",
    "batch_size = 8 # Рекоммендуемы размер батча на Colab\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset, shuffle=True, batch_size=batch_size, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756454f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prepare_batch(batch, pipe):\n",
    "    \"\"\"\n",
    "    Preprocess a batch of textual prompts and images to corresponding embeddings,\n",
    "    using the text encoder and VAE\n",
    "    Params:\n",
    "    \n",
    "    Return:\n",
    "        latents: torch.Tensor([B, 4, 64, 64], dtype=torch.float16)\n",
    "        prompt_embeds: torch.Tensor([B, 77, D], dtype=torch.float16)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize prompts\n",
    "    text_inputs = pipe.tokenizer(\n",
    "        batch['text'],\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Exctract prompt embeddings using the text encoder\n",
    "    prompt_embeds = pipe.text_encoder(text_inputs.input_ids.cuda())[0]\n",
    "\n",
    "    # Map images to the VAE latent space\n",
    "    image = batch['image'].to(\"cuda\", dtype=torch.float16)\n",
    "    latents = pipe.vae.encode(image).latent_dist.sample()\n",
    "    latents = latents * pipe.vae.config.scaling_factor\n",
    "    return latents, prompt_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53425e2f",
   "metadata": {},
   "source": [
    "### Подготовка дискриминатора (1 балл)\n",
    "\n",
    "Дискриминатор у нас будет классификационной головой поверх фичей учителя, как в [LADD](https://arxiv.org/abs/2403.12015). \n",
    "\n",
    "* Фичи мы будем извлекать с помощью forward hook-ов в торче. Функции хуков вам даны ниже. Вам нужно корректно применить их. Извлекать фичи будем из mid-block-a UNet-a.\n",
    "\n",
    "* Архитектура MLP головы вам уже дана для экономии времени. FYI, она не обязательная должна быть именно такая. Просто мы взяли её из наших реализаций.   \n",
    "\n",
    "* В классе дискриминатора нужно эту голову корректно применить к извлеченным фичам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tensors(module: nn.Module, features, name: str):\n",
    "    \"\"\" Обработка и сохранение активаций в моделе \"\"\"\n",
    "    if type(features) in [list, tuple]:\n",
    "        features = [f.float() if f is not None else None \n",
    "                    for f in features]\n",
    "        setattr(module, name, features)\n",
    "    elif isinstance(features, dict):\n",
    "        features = {k: f.float() for k, f in features.items()}\n",
    "        setattr(module, name, features)\n",
    "    else:\n",
    "        setattr(module, name, features.float())\n",
    "\n",
    "        \n",
    "def save_out_hook(self, inp, out):\n",
    "    \"\"\" Cохраняет входы в модуль \"\"\"\n",
    "    save_tensors(self, out, 'activations')\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_input_hook(self, inp, out):\n",
    "    \"\"\" Cохраняет выходы из модуля \"\"\"\n",
    "    save_tensors(self, inp[0], 'activations')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccc65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForward(dim, outdim=None, mult=1):\n",
    "    outdim = dim if outdim is None else outdim\n",
    "    return nn.Sequential(\n",
    "        nn.LayerNorm(dim),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(dim, outdim),\n",
    "    )\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, teacher_unet, num_discriminator_layers=4):\n",
    "        super().__init__()\n",
    "        self.unet = teacher_unet\n",
    "\n",
    "        dimensions = torch.linspace(\n",
    "            teacher_unet.mid_block.attentions[0].norm.weight.shape[0],\n",
    "            1,\n",
    "            num_discriminator_layers + 1,\n",
    "            dtype=int\n",
    "        )\n",
    "        \n",
    "        # Создаем классификационную голову для дискриминатора\n",
    "        self.list_of_layers = []\n",
    "        for j, dim in enumerate(dimensions[:-1]):\n",
    "            self.list_of_layers.append(FeedForward(dim.item(), dimensions[j+1].item()))\n",
    "            \n",
    "        self.cls_pred_branch = nn.Sequential(*self.list_of_layers)\n",
    "        \n",
    "        num_cls_params = sum(p.numel() for p in self.cls_pred_branch.parameters())\n",
    "        print(f'Classification head number of trainable params: {num_cls_params}')\n",
    "        \n",
    "        # Зарегистрируйте форвард хук\n",
    "        <YOUR CODE HERE>\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        # Прогоняем модель учителя, чтобы извлечь фичи\n",
    "        self.unet(*args, **kwargs)\n",
    "        \n",
    "        # Извлекаем фичи и вычисляем логиты. Важно конвертировать фичи из FP16 в FP32\n",
    "        <YOUR CODE HERE>\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    'sd-legacy/stable-diffusion-v1-5', subfolder=\"unet\",\n",
    ").cuda().train()\n",
    "\n",
    "assert unet.dtype == torch.float32\n",
    "assert unet.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7bc989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указываем к каким модулям модели мы будет добавлять адаптеры.\n",
    "lora_modules = [\n",
    "    \"to_q\", \"to_k\", \"to_v\", \"to_out.0\", \"proj_in\", \"proj_out\",\n",
    "    \"ff.net.0.proj\", \"ff.net.2\", \"conv1\", \"conv2\", \"conv_shortcut\",\n",
    "    \"downsamplers.0.conv\", \"upsamplers.0.conv\", \"time_emb_proj\"\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64, # задает ранг у матриц A и B в LoRA.\n",
    "    target_modules=lora_modules\n",
    ")\n",
    "\n",
    "\n",
    "# Создаем обертку исходной UNet модели с LoRA адаптерами, используя библиотеку PEFT\n",
    "unet = get_peft_model(unet, lora_config, adapter_name=\"student\")\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# Создаем дискриминатор и соотвествующий оптимизатор\n",
    "D_unet = Discriminator(teacher_unet).cuda()\n",
    "D_optimizer = torch.optim.AdamW(D_unet.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# Включаем gradient checkpointing - важная техника для экономии памяти во время обучения\n",
    "unet.enable_gradient_checkpointing()\n",
    "teacher_unet.enable_gradient_checkpointing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc217018",
   "metadata": {},
   "source": [
    "## Adversarial Diffusion Distillation (1 балл + 1 балл бонусом)\n",
    "\n",
    "Здесь вам нужно реализовать ГАН лосс и функцию для предиктов студентом. ГАН лосс может быть любым на ваш вкус.\n",
    "\n",
    "После этого нужно дописать цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dff27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_loss_fn(fake_logits, real_logits=None):\n",
    "    \"\"\" GAN лосс для дискриминатора и генератора \"\"\"\n",
    "    if real_logits is not None:\n",
    "        # Discriminator loss\n",
    "        <YOUR CODE HERE>\n",
    "    else:\n",
    "        # Generator loss\n",
    "        <YOUR CODE HERE>\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1676632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x0_from_noise(sample, model_output, alphas_cumprod, timestep):\n",
    "    \"\"\" Получение х_0 из x_t и предсказанного шума \"\"\"\n",
    "    <YOUR CODE HERE>\n",
    "    return pred_original_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_prediction(\n",
    "    latents,\n",
    "    prompt_embeds,\n",
    "    unet, \n",
    "    scheduler,\n",
    "    num_timesteps=1000,\n",
    "    student_timesteps=[249, 499, 749, 999],\n",
    "):\n",
    "    # Сэмплируем x_t для студента из student_timesteps\n",
    "    if isinstance(student_timesteps, list):\n",
    "        student_timesteps = torch.tensor(student_timesteps, device=latents.device)\n",
    "        \n",
    "    index = torch.randint(\n",
    "        0, len(student_timesteps), \n",
    "        (len(latents),), \n",
    "        device=latents.device\n",
    "    ).long()\n",
    "    \n",
    "    timesteps = student_timesteps[index]\n",
    "    \n",
    "    # Получаем зашумленные латенты\n",
    "    noisy_latents = <YOUR CODE HERE>\n",
    "\n",
    "    # Сэмплируем студентом \n",
    "    # with <YOUR CODE HERE>: # для реализации mixed-precision обучения\n",
    "    noise_pred = <YOUR CODE HERE>\n",
    "    \n",
    "    x0_pred = <YOUR CODE HERE>\n",
    "    return x0_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbabec36",
   "metadata": {},
   "source": [
    "### Обучающий цикл\n",
    "\n",
    "Вам дан код обучения модель в полной точности (FP32) c батчом 8. В ячейке ниже вам нужно добавить mixed precision FP16/FP32 по аналогии с другими домашками. **Обратите внимание, что mixed-precision нужен только для обновления генератора**\n",
    "\n",
    "Про реализацию mixed-precision в pytorch можно перейти по ссылке: [Mixed-precision обучение](https://pytorch.org/docs/stable/notes/amp_examples.html#typical-mixed-precision-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e13dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "discriminator_iters = 1\n",
    "\n",
    "for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "    \n",
    "    # Сэмплируем батч из датасета\n",
    "    latents, prompt_embeds = prepare_batch(batch, pipe)\n",
    "    \n",
    "    # Обновляем дискриминатор\n",
    "    D_unet.cls_pred_branch.requires_grad_(True) # Включаем градиенты до головы\n",
    "    for _ in range(discriminator_iters):\n",
    "        with torch.no_grad():\n",
    "            fake_latents = <YOUR CODE HERE>\n",
    "        \n",
    "        # Вычисляем логиты из дискриминатора. \n",
    "        # Не забудьте, что учитель внутри дискриминатора в FP16\n",
    "        # Для дискриминатора будем использова t=0\n",
    "        D_timesteps = torch.zeros(len(latents), device=latents.device)\n",
    "        \n",
    "        fake_logits = <YOUR CODE HERE>\n",
    "\n",
    "        real_logits = <YOUR CODE HERE>\n",
    "        \n",
    "        gan_cls_loss = <YOUR CODE HERE>\n",
    "        \n",
    "        # Обновляем параметры fake модели\n",
    "        gan_cls_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        D_optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    \n",
    "    # Сэмплируем латенты для обновления студента. Уже без torch.no_grad()\n",
    "    fake_latents = <YOUR CODE HERE>\n",
    "    \n",
    "    D_unet.cls_pred_branch.requires_grad_(False) # Выключаем градиенты до головы\n",
    "    fake_logits = <YOUR CODE HERE>\n",
    "    gan_gen_loss = <YOUR CODE HERE>\n",
    "    \n",
    "    # Обновляем параметры\n",
    "    gan_gen_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    print(f\"G loss: {gan_gen_loss.item():.3f} | D loss: {gan_cls_loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76207397",
   "metadata": {},
   "source": [
    "### Генерация с помощью multistep stochastic sampling\n",
    "\n",
    "Генерируем картинки с помощью нашей модели. Нам нужен специальный сэмплер, который схематично изображен на картинке ниже. \n",
    "\n",
    "**Эту функцию вам нужно взять из Части 1. Там она называется consistency_sampling.**\n",
    "\n",
    "<div>\n",
    "<img src=\"https://i.postimg.cc/66bWLvnh/cd-sampling.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Чуть более формально:\n",
    "\n",
    "$x_{t_n} \\sim \\mathcal{N}(0, I)$\n",
    "\n",
    "$for\\ t_i \\in [t_n, ..., t_1]:$\n",
    "\n",
    "* $\\epsilon \\leftarrow unet(x_{t_i})$\n",
    "\n",
    "* $x_0 \\leftarrow DDIM(\\epsilon, x_{t_i}, t_i, 0)$\n",
    "\n",
    "* $x_{t_{i-1}} \\leftarrow q(x_{t_{i-1}} | x_0)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68edc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# СОВПАДАЕТ С consistency_sampling ИЗ ЧАСТИ 1. МОЖНО СКОПИРОВАТЬ ОТТУДА\n",
    "\n",
    "@torch.no_grad()\n",
    "def multistep_sampling(\n",
    "    pipe,\n",
    "    prompt,\n",
    "    num_inference_steps=4,\n",
    "    generator=None,\n",
    "    num_images_per_prompt=4,\n",
    "    guidance_scale=1\n",
    "):\n",
    "    if prompt is not None and isinstance(prompt, str):\n",
    "        batch_size = 1\n",
    "    elif prompt is not None and isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "\n",
    "    device = pipe._execution_device\n",
    "\n",
    "    # Извлекаем эмбеды из текстовых промптов. Реализуйте вызов pipe.encode_prompt\n",
    "    do_classifier_free_guidance = guidance_scale > 0\n",
    "    prompt_embeds, null_prompt_embeds = <YOUR CODE HERE>\n",
    "    assert prompt_embeds.dtype == null_prompt_embeds.dtype == torch.float16\n",
    "\n",
    "    # Настраиваем параметры scheduler-a\n",
    "    assert pipe.scheduler.config['timestep_spacing'] == 'trailing'\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    # Создаем батч латентов из N(0,I)\n",
    "    latents = <YOUR CODE HERE>\n",
    "\n",
    "    for i, t in enumerate(tqdm(pipe.scheduler.timesteps)):\n",
    "        t = torch.tensor([t] * len(latents)).to(device)\n",
    "        zero_t = torch.tensor([0] * len(latents)).to(device)\n",
    "\n",
    "        cond_noise_pred = <YOUR CODE HERE>\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            uncond_noise_pred = <YOUR CODE HERE>\n",
    "            noise_pred = <YOUR CODE HERE>\n",
    "        else:\n",
    "            noise_pred = cond_noise_pred\n",
    "\n",
    "        # Получаем x_0 оценку из x_t\n",
    "        x_0 = <YOUR CODE HERE>\n",
    "\n",
    "        if i + 1 < num_inference_steps:\n",
    "            # Переход на следующий шаг\n",
    "            s = pipe.scheduler.timesteps[i+1]\n",
    "            s = torch.tensor([s] * len(latents)).to(device)\n",
    "\n",
    "            latents = <YOUR CODE HERE>\n",
    "        else:\n",
    "            # Последний шаг\n",
    "            latents = x_0\n",
    "\n",
    "        latents = latents.half()\n",
    "\n",
    "    # Декодируем латенты в пиксели. Не забудьте про pipe.vae.config.scaling_factor \n",
    "    image = <YOUR CODE HERE>\n",
    "    do_denormalize = [True] * image.shape[0]\n",
    "    image = pipe.image_processor.postprocess(image, output_type=\"pil\", do_denormalize=do_denormalize)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956dbb7d",
   "metadata": {},
   "source": [
    "### Сгенерируем картинки для разных промптов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ccbbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_prompts = [\n",
    "    \"A sad puppy with large eyes\",\n",
    "    \"A girl with pale blue hair and a cami tank top\",\n",
    "    \"A lighthouse in a giant wave, origami style\",\n",
    "    \"belle epoque, christmas, red house in the forest, photo realistic, 8k\",\n",
    "    \"A small cactus with a happy face in the Sahara desert\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef362c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet = unet.eval().to(torch.float16)\n",
    "assert unet.active_adapter == 'student'\n",
    "\n",
    "guidance_scale = 1\n",
    "\n",
    "for prompt in validation_prompts:\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(1)\n",
    "\n",
    "    # Применяем консистенси сэмплирование.\n",
    "    images = <YOUR CODE HERE>\n",
    "\n",
    "    visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d7f668",
   "metadata": {},
   "source": [
    "**Важно:** ваши результаты не должны совпадать с теми что ниже. Это скорее ориентир по качеству. Вполне вероятно у вас могут получиться картинки сильно лучше :)\n",
    "\n",
    "**Референсные примеры CFG=1**\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/ysda-hw2-add-references/reference_add_dog.png)\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/ysda-hw2-add-references/reference_add_girl_cfg1.png)\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/ysda-hw2-add-references/reference_add_house_cfg1.png)\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/ysda-hw2-add-references/reference_add_lighthouse_cfg1.png)\n",
    "\n",
    "\n",
    "**Референсные примеры CFG=2**\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/ysda-hw2-add-references/reference_add_dog_cfg2.png)\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/ysda-hw2-add-references/reference_add_girl_cfg2.png)\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/ysda-hw2-add-references/reference_add_house_cfg2.png)\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/ysda-hw2-add-references/reference_add_lighthouse_cfg2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84190e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
