{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bb8ead",
   "metadata": {
    "id": "c3bb8ead"
   },
   "source": [
    "# ДЗ №2 | Часть 1 | Консистенси модели\n",
    "\n",
    "\n",
    "Мы разбирали как дистиллировать многошаговую диффузионную модель в малошагового студента, и тем самым будет работать на порядок быстрее учителя.\n",
    "\n",
    "Один из подходов, который мы разбирали *Consistency Distillation*. В этом задании, мы закрепим материал, который был на лекции и семинаре и реализуем этот фреймворк, затрагивая различные нюансы.\n",
    "\n",
    "В этом задании мы будем дистиллировать модель *Stable Diffusion 1.5 (SD1.5)* для генерации картинок по текстовому описанию.\n",
    "\n",
    "В результате выполнения задания мы придем к неплохой модели для генерации картинок за 4 шага, работая в ограниченных условиях колаба."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e5265",
   "metadata": {
    "id": "585e5265"
   },
   "outputs": [],
   "source": [
    "!pip install -U diffusers --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4aafa",
   "metadata": {
    "id": "67e4aafa"
   },
   "source": [
    "## Теормин\n",
    "\n",
    "---\n",
    "### Диффузионные модели\n",
    "\n",
    "Задан прямой диффузионный процесс, который переводит чистые картинки в шум с помощью распределения $q(\\mathbf{x}_t | \\mathbf{x}_0)= {N}(\\mathbf{x}_t | \\alpha_t \\mathbf{x}_0, \\sigma^2_t I)$\n",
    "\n",
    "Таким образом, мы можем получаться зашумленные картинки по следующей формуле: $\\mathbf{x}_t = \\alpha_t \\mathbf{x}_0 + \\sigma_t \\epsilon$, где $\\epsilon{\\sim} {N}(0, I)$ **(1)**\n",
    "\n",
    "$\\alpha_t, \\sigma_t$ задают процесс зашумления. Здесь мы будем иметь дело с *variance preserving (VP)* процессом: $\\alpha^2_t = 1 - \\sigma^2_t$.\n",
    "\n",
    "Диффузионная модель (ДМ) пытается решить обратную задачу: из шума порождать новые картинки.\n",
    "Важно, что диффузионный процесс можно описать следующим обыкновенным дифференциальным уравнением (ОДУ):\n",
    "\n",
    " $dx = \\left[ f(\\mathbf{x}, t) - \\frac{1}{2} g(t)^2 \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}) \\right] dt$, **(2)**\n",
    "\n",
    "где $f(\\mathbf{x}, t)$ известен из заданного процесса зашумления, а $\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t)$ (*скор функцию*) оцениваем с помощью нейросети: $s_\\theta(\\mathbf{x}_t, t) \\approx \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t)$. Таким образом, имея оценку на $\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x})$, мы можем решить это ОДУ, стартуя со случайного шума, и получить картинку.\n",
    "\n",
    "\n",
    "**SD1.5** использует *$\\epsilon$-параметризацию*, т.е., UNet пытается предсказать шум, который мы добавили на картинку по формуле **(1)**. Оценку скор функции можно получить, пользуясь результатом, вытекающим из формулы Твидди: $s_\\theta(\\mathbf{x}_t, t) = - \\frac{\\epsilon_\\theta(\\mathbf{x}_t, t)} { \\sigma_t}$\n",
    "\n",
    "Чтобы решить ОДУ **(2)**, нам нужно воспользоваться каким-то численным методом (солвером). В этом задании мы будем работать с не самым эффектным, но самым популярным солвером: **DDIM**, который является адаптированным методом Эйлера под диффузионный ОДУ.\n",
    "\n",
    "Для VP процесса переход с помощью DDIM с шага $t$ на $s$ можно сделать следующим образом:\n",
    "\n",
    "$\n",
    "x_s = DDIM(\\mathbf{x}_t, t, s) = \\alpha_s \\cdot \\left(\\frac{\\mathbf{x}_t - \\sigma_t \\epsilon_\\theta}{\\alpha_t} \\right) + \\sigma_s \\epsilon_\\theta\n",
    "$\n",
    "\n",
    "Этот переход можно интерпретировать так: получаем оценку на чистую картинку $\\mathbf{x}_0$ на шаге $t$, используя $\\frac{\\mathbf{x}_t - \\sigma_t \\epsilon_\\theta}{\\alpha_t}$, а потом снова зашумляем эту оценку на шаг $s$ по формуле **(1)**, но только используем не случайный шум, а шум предсказанный моделью $\\epsilon_\\theta$.\n",
    "\n",
    "*Используя DDIM для SD1.5, можем получать хорошие картинки за 50 шагов.*\n",
    "\n",
    "**SD1.5** - латентная ДМ, т.е. модель работает не в пиксельном пространстве, а в латентном пространстве **VAE**. Таким образом SD1.5 состоит из следующих компонент:\n",
    "\n",
    "1) **VAE** - переводит $3{\\times}512{\\times}512$ картинки в латенты $4{\\times}64{\\times}64$ и может декодировать их обратно в картинки.\n",
    "2) **Текстовый энкодер** - извлекает текстовые признаки из промпта. Эти признаки будут подаваться в диффузионную модель, чтобы дать модели информацию, что именно хотим сгенерировать\n",
    "3) **Диффузионная модель** - UNet, работающий на \"латентных картинках\" $4{\\times}64{\\times}64$.\n",
    "\n",
    "---\n",
    "### [Консистенси модели](https://arxiv.org/pdf/2303.01469)\n",
    "\n",
    "Главная цель дистилляции диффузии - уменьшить количество шагов ДМ, при этом сохранив высокое качество картинок.\n",
    "\n",
    "**Консистенси модели (Consistency Models | CM)** - класс моделей, где мы хотим выучить \"консистенси функцию\" $f_\\theta(\\mathbf{x}_t)$ - с любой точки $\\mathbf{x}_{t}$ траектории диффузионного ОДУ **(2)** сразу предсказывать $\\mathbf{x}_{0}$ (чистые данные) за один шаг. \n",
    "Если мы идеально выучим консистенси функцию, то сможем шагать из чистого шума сразу в картинку, что супер эффективно в отличии от генерации ДМ. \n",
    "\n",
    "Отметим, что консистенси модель можно учить как независимую генеративную модель, без предобученной ДМ, и в *задании 3* вам предстоит подумать, как это можно сделать.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div>\n",
    "\n",
    "<img src=\"https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/cd-idea.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "**Консистенси дистилляция (Consistency Distillation | CD)** - подход, когда для обучения CM, мы используем предобученную ДМ. ДМ нам дает качественную инициализацию модели и уже обученную скор функцию, что сильно упрощает сходимость консистенси моделей. \n",
    "\n",
    "\n",
    "#### Обучение CM\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/cd-training.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Главный принцип обучения консистенси моделей заключается в попытке удовлетворить *self-consistency* св-ву: выход CM на двух соседних точках траектории $\\mathbf{x}_{t}$ и $\\mathbf{x}_{t-1}$ должен совпадать по какой-то мере близости, например L2 расстояние: $\\lVert f_\\theta(\\mathbf{x}_{t-1}) - f_\\theta(\\mathbf{x}_{t}) \\rVert^2_2$.\n",
    "\n",
    "Заметим, что self-consistency св-во удовлетворить очень просто без какого-либо обучения, взяв, например $f_\\theta(\\mathbf{x}_{t}) \\equiv 0$.\n",
    "\n",
    "Поэтому, чтобы избежать вырожденных решений, нам необходимо выставить граничное условие (boundary condition), которое будет требовать, чтобы в самой левой точке траектории около $t=0$, модель предсказывала картинку, которую получает на вход: $f_\\theta(\\mathbf{x}_0) = \\mathbf{x}_0$.\n",
    "\n",
    "**Практическое замечание:** Для обеих точек траектории мы применяем одну и ту же модель $f_\\theta(\\cdot)$. Но выход модели на шаге ${t-1}$ является \"таргетом\" для выхода модели на шаге $t$ и поэтому выполнение модели для шага $t-1$ делается в *torch.no_grad* режиме.\n",
    "\n",
    "**Как получать две соседние точки на траектории ОДУ?**\n",
    "\n",
    "Берем случайную картинку $\\mathbf{x}_0$ из датасета.\n",
    "\n",
    "Точку $\\mathbf{x}_t$ получаем с помощью прямого процесса зашумления: $\\mathbf{x}_t = q(\\mathbf{x}_t | \\mathbf{x}_0)$\n",
    "\n",
    "Чтобы получить соседнюю точку $\\mathbf{x}_{t-1}$, нам нужно сделать шаг по траектории ОДУ, используя, например, DDIM солвер. \n",
    "\n",
    "В консистенси дистилляции, мы делаем шаг предобученной ДМ: $\\mathbf{x}_{t-1} = DDIM(\\epsilon_\\theta(\\mathbf{x}_t, t), \\mathbf{x}_t, t, t-1)$\n",
    "\n",
    "**Важно:** на практике мы можем брать не соседние шаги $t$ и $t-1$, а с некоторым интервалом, например 20 шагов. \n",
    "Размер интервала влияет на bias/variance trade-off в консистенси обучении: больше интервал между шагами - больше смещение, но меньше дисперсия, и наоборот. \n",
    "Для простоты в этом задании мы зафиксируем интервал - 20 шагов, но во многих работах размер интервала динамически меняется по ходу обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18dffaf",
   "metadata": {
    "id": "c18dffaf"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline, LCMScheduler, UNet2DConditionModel, DDIMScheduler\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, get_peft_model_state_dict\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d0a3a",
   "metadata": {
    "id": "669d0a3a"
   },
   "outputs": [],
   "source": [
    "#---------------------\n",
    "# Visualization utils\n",
    "#---------------------\n",
    "\n",
    "def visualize_images(images):\n",
    "    assert len(images) == 4\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=-0.01, hspace=-0.01)\n",
    "\n",
    "\n",
    "#--------------\n",
    "# Tensor utils\n",
    "#--------------\n",
    "\n",
    "def extract_into_tensor(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "#---------------\n",
    "# Dataset utils\n",
    "#---------------\n",
    "\n",
    "class TextImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, subset_name=\"train2014_5k\", transform=None, max_cnt=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            root_dir (string): Директория с картинками\n",
    "            transform (callable, optional): преобразования, применимые к картинкам\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.extensions = (\n",
    "            \".jpg\",\n",
    "            \".jpeg\",\n",
    "            \".png\",\n",
    "            \".ppm\",\n",
    "            \".bmp\",\n",
    "            \".pgm\",\n",
    "            \".tif\",\n",
    "            \".tiff\",\n",
    "            \".webp\",\n",
    "        )\n",
    "        sample_dir = os.path.join(root_dir, subset_name)\n",
    "\n",
    "        # Собираем пути до картинок\n",
    "        self.samples = sorted(\n",
    "            [\n",
    "                os.path.join(sample_dir, fname)\n",
    "                for fname in os.listdir(sample_dir)\n",
    "                if fname[-4:] in self.extensions\n",
    "            ],\n",
    "            key=lambda x: x.split(\"/\")[-1].split(\".\")[0],\n",
    "        )\n",
    "        self.samples = (\n",
    "            self.samples if max_cnt is None else self.samples[:max_cnt]\n",
    "        )  # \n",
    "\n",
    "        # Собираем промпты\n",
    "        self.captions = {}\n",
    "        with open(\n",
    "            os.path.join(root_dir, f\"{subset_name}.csv\"), newline=\"\\n\"\n",
    "        ) as csvfile:\n",
    "            spamreader = csv.reader(csvfile, delimiter=\",\")\n",
    "            for i, row in enumerate(spamreader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                self.captions[row[1]] = row[2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample_path = self.samples[idx]\n",
    "        sample = Image.open(sample_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return {\n",
    "            \"image\": sample,\n",
    "            \"text\": self.captions[os.path.basename(sample_path)],\n",
    "            \"idxs\": idx, }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62860269",
   "metadata": {
    "id": "62860269"
   },
   "source": [
    "# Модель учителя (SD1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce973eb",
   "metadata": {
    "id": "8ce973eb"
   },
   "source": [
    "## Задание №0\n",
    "\n",
    "Давайте для начала загрузим модель [StableDiffusion 1.5](https://huggingface.co/sd-legacy/stable-diffusion-v1-5) и сгенерируем ей картинки за 50 шагов.\n",
    "\n",
    "**Важно:** для экономии памяти, загружаем все компоненты модели в FP16. Не забываем положить модель на GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46016d",
   "metadata": {
    "id": "4e46016d"
   },
   "outputs": [],
   "source": [
    "pipe = <YOUR CODE HERE>\n",
    "\n",
    "# Проверяем, что все компоненты модели в FP16 и на cuda\n",
    "assert pipe.unet.dtype == torch.float16 and pipe.unet.device.type == 'cuda'\n",
    "assert pipe.vae.dtype == torch.float16 and pipe.vae.device.type == 'cuda'\n",
    "assert pipe.text_encoder.dtype == torch.float16 and pipe.text_encoder.device.type == 'cuda'\n",
    "\n",
    "# Заменяем дефолтный сэмплер на DDIM\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "pipe.scheduler.timesteps = pipe.scheduler.timesteps.cuda()\n",
    "pipe.scheduler.alphas_cumprod = pipe.scheduler.alphas_cumprod.cuda()\n",
    "\n",
    "# Отдельно извлечем модель учителя, которую потом будем дистиллировать\n",
    "teacher_unet = pipe.unet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb66c9",
   "metadata": {
    "id": "20cb66c9"
   },
   "source": [
    "Теперь сгенерируем картинки за 50 шагов. Вам нужно написать вызов pipe и передать в него промпт, число шагов генерации, генератор случайных чисел, гайденс скейл и указать, чтобы сгенерировалось 4 картинки на промпт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb382b",
   "metadata": {
    "id": "d5bb382b"
   },
   "outputs": [],
   "source": [
    "prompt = \"A sad puppy with large eyes\"\n",
    "guidance_scale = 7.5\n",
    "generator = torch.Generator('cuda').manual_seed(1)\n",
    "\n",
    "images = <YOUR CODE HERE>\n",
    "\n",
    "visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8a547",
   "metadata": {
    "id": "09c8a547"
   },
   "source": [
    "Давайте посмотрим, что выдаст модель за 4 шага.\n",
    "Все то же самое, что и выше, просто поменяем число шагов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e66279",
   "metadata": {
    "id": "06e66279"
   },
   "outputs": [],
   "source": [
    "generator = torch.Generator('cuda').manual_seed(1)\n",
    "\n",
    "images = <YOUR CODE HERE>\n",
    "\n",
    "visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb5003",
   "metadata": {
    "id": "b0cb5003"
   },
   "source": [
    "На 4 шагах картинки получаются размазанными. Давайте постараемся починить их."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82774c59",
   "metadata": {
    "id": "82774c59"
   },
   "source": [
    "##  Создаем датасет\n",
    "\n",
    "Мы будем дообучать модель на небольшой обучающей выборке из 10000 пар текст-картинка сгенерированные моделью [FLUX](https://huggingface.co/black-forest-labs/FLUX.1-dev).\n",
    "\n",
    "Данные можно загрузить с помощью команд в ячейке ниже. В текущей директории ./ должны появиться:\n",
    "* Папка flux_data с 10000 картинками\n",
    "* Файл flux_data.csv с 10000 промптами\n",
    "\n",
    "Данные парсятся корректным образом в уже реализованном классе **TextImageDataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431a4e8",
   "metadata": {
    "id": "2431a4e8"
   },
   "outputs": [],
   "source": [
    "!wget https://storage.yandexcloud.net/yandex-research/flux_data_10k.tar.gz\n",
    "!tar -xzf flux_data_10k.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc010814",
   "metadata": {
    "id": "bc010814"
   },
   "source": [
    "**Важное замечание:** для более быстрого дебаггинга можете взять, например, 1000 картинок и прогнать на бОльшей выборке только в самом конце. 1000-2500 картинок должно быть достаточно для понимания корректно ли реализованы функции.\n",
    "Совсем для первичного дебаггинга можно взять еще меньше картинок.\n",
    "\n",
    "**Для финального запуска достаточно выставить max_cnt=5000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba81fb",
   "metadata": {
    "id": "e1ba81fb"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(512),\n",
    "        transforms.CenterCrop(512),\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: 2 * x - 1,\n",
    "    ]\n",
    ")\n",
    "dataset = TextImageDataset(\".\",\n",
    "    subset_name=\"flux_data\",\n",
    "    transform=transform,\n",
    "    max_cnt=5000 # Для дебага можно взять 1000 или меньше\n",
    ")\n",
    "\n",
    "batch_size = 8 # Рекоммендуемы размер батча на Colab\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset, shuffle=True, batch_size=batch_size, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad222c2",
   "metadata": {
    "id": "cad222c2"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prepare_batch(batch, pipe):\n",
    "    \"\"\"\n",
    "    Предобработка батча картинок и текстовых промптов.\n",
    "    Маппим картинки в латентное пространство VAE.\n",
    "    Извлекаем эмбеды промптов с помощью текстового энкодера.\n",
    "    \n",
    "    Params:\n",
    "\n",
    "    Return:\n",
    "        latents: torch.Tensor([B, 4, 64, 64], dtype=torch.float16)\n",
    "        prompt_embeds: torch.Tensor([B, 77, D], dtype=torch.float16)\n",
    "    \"\"\"\n",
    "\n",
    "    # Токенизируем промпты\n",
    "    text_inputs = pipe.tokenizer(\n",
    "        batch['text'],\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Извлекаем эмбеды промптов с помощью текстового энкодера\n",
    "    prompt_embeds = pipe.text_encoder(text_inputs.input_ids.cuda())[0]\n",
    "\n",
    "    # Переводим картинки в латентное пространство VAE\n",
    "    image = batch['image'].to(\"cuda\", dtype=torch.float16)\n",
    "    latents = pipe.vae.encode(image).latent_dist.sample()\n",
    "    latents = latents * pipe.vae.config.scaling_factor\n",
    "    return latents, prompt_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a00a9",
   "metadata": {
    "id": "062a00a9"
   },
   "source": [
    "### Подготовка моделей и оптимизатора\n",
    "\n",
    "Для начала создаем обучаемую модель: UNet инициализируемый весами SD1.5.\n",
    "Вам нужно воспользоваться классом UNet2DConditionModel и загрузить отдельно только UNet модель из SD1.5.\n",
    "\n",
    "Отметим, что эта модель у нас будет храниться в полной точности FP32, потому что обучение параметров в FP16 может приводить к нестабильностям и низкому качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e73f1a",
   "metadata": {
    "id": "e2e73f1a"
   },
   "outputs": [],
   "source": [
    "unet = <YOUR CODE HERE>\n",
    "\n",
    "assert unet.dtype == torch.float32\n",
    "assert unet.training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f56cb",
   "metadata": {
    "id": "cd4f56cb"
   },
   "source": [
    "Для экономии памяти во время обучения будем учить не параметры самой модели, а добавим в нее обучаемые LoRA адаптеры с малым числом параметров.\n",
    "\n",
    "LoRA представляет собой маленькую добавку к весам модели, где на одну матрицу весов $W \\in \\mathbb{R}^{m{\\times}n} $ обучаются две низкоранговые матрицы $W_A \\in \\mathbb{R}^{k{\\times}n}$ и $W_B \\in \\mathbb{R}^{k{\\times}m}$, где $k$ - ранг матрицы сильно меньше $m$ и $n$.\n",
    "\n",
    "Тем самым, новая обученная матрица весов может быть представлена как $\\hat{W} = W + \\Delta W = W + W^T_B W_A$.  \n",
    "Во время инференса $\\Delta W$ можно вмержить в $W$ и получить итоговую модель. \n",
    "Также частая практика оставлять адаптеры как есть, чтобы была возможность для одной базовой модели учить несколько адаптеров под разные задачи и переключаться между ними по необходимости.\n",
    "\n",
    "Если не мержить адаптеры, то вычисления для линейного слоя происходят как на картинке ниже.\n",
    "\n",
    "<img src=https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/lora-idea.jpg width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020bb504",
   "metadata": {
    "id": "020bb504"
   },
   "outputs": [],
   "source": [
    "# Указываем к каким слоям модели мы будет добавлять адаптеры.\n",
    "lora_modules = [\n",
    "    \"to_q\", \"to_k\", \"to_v\", \"to_out.0\", \"proj_in\", \"proj_out\",\n",
    "    \"ff.net.0.proj\", \"ff.net.2\", \"conv1\", \"conv2\", \"conv_shortcut\",\n",
    "    \"downsamplers.0.conv\", \"upsamplers.0.conv\", \"time_emb_proj\"\n",
    "]\n",
    "lora_config = LoraConfig(\n",
    "    r=64, # задает ранг у матриц A и B в LoRA.\n",
    "    lora_alpha=1, # контролирует LR с которым учим LoRA: lr_lora = lr_base * lora_alpha / r. Если учится только LoRA, то можно просто менять LR в оптимизаторе\n",
    "    target_modules=lora_modules\n",
    ")\n",
    "\n",
    "# Создаем обертку исходной UNet модели с LoRA адаптерами, используя библиотеку PEFT\n",
    "cm_unet = get_peft_model(unet, lora_config, adapter_name=\"ct\")\n",
    "\n",
    "# Включаем gradient checkpointing - важная техника для экономии памяти во время обучения\n",
    "cm_unet.enable_gradient_checkpointing()\n",
    "\n",
    "# Создаем оптимизатор\n",
    "optimizer = torch.optim.AdamW(cm_unet.parameters(), lr=1e-4)\n",
    "\n",
    "# Задаем лосс функцию для CM обжектива. В базовом варианте разумно взять L2\n",
    "# По умолчанию, она уже выдает усредненное значение по всем размерностям\n",
    "mse_loss = torch.nn.functional.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf9615",
   "metadata": {
    "id": "52bf9615"
   },
   "source": [
    "## Задание №1 (0.5 балл)\n",
    "\n",
    "####  Реализация шага DDIM\n",
    "\n",
    "Шаг с помощью DDIM с $\\mathbf{x}_t$ на $\\mathbf{x}_s$ можно сделать следующим образом:\n",
    "\n",
    "$\n",
    "\\mathbf{x}_s = DDIM(\\epsilon_\\theta, \\mathbf{x}_t, t, s) = \\alpha_s \\cdot \\left(\\frac{\\mathbf{x}_t - \\sigma_t \\epsilon_\\theta}{\\alpha_t} \\right) + \\sigma_s \\epsilon_\\theta\n",
    "$\n",
    "\n",
    "Вам нужно реализовать эту формулу в уже готовом шаблоне ниже.\n",
    "Чтобы корректно выполнить задание, вам нужно задать $\\alpha_t$ и $\\sigma_t$ имея *DDIMScheduler*.\n",
    "**Обратите внимание на аттрибут *scheduler.alphas_cumprod***, который задает $\\bar\\alpha_{t} = \\prod^t_{i=1} (1-\\beta_i)$ в классической DDPM формулировке: [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743504b",
   "metadata": {
    "id": "d743504b"
   },
   "outputs": [],
   "source": [
    "def ddim_solver_step(model_output, x_t, t, s, scheduler):\n",
    "    \"\"\"\n",
    "    Шаг DDIM солвера для VP процесса зашумления и eps-prediction модели\n",
    "    params:\n",
    "        model_output: torch.Tensor[B, 4, 64, 64] - предсказание модели - шум eps\n",
    "        x_t: torch.Tensor[B, 4, 64, 64] - сэмплы на шаге t\n",
    "        t: torch.Tensor[B] - номер текущего шага\n",
    "        s: torch.Tensor[B] - номер следующего шага\n",
    "        scheduler: DDIMScheduler - расписание диффузионного процесса, чтобы получить alpha и sigma\n",
    "    \"\"\"\n",
    "    alphas = <YOUR CODE HERE>\n",
    "    sigmas = <YOUR CODE HERE>\n",
    "\n",
    "    sigmas_s = extract_into_tensor(sigmas, s, x_t.shape)\n",
    "    alphas_s = extract_into_tensor(alphas, s, x_t.shape)\n",
    "\n",
    "    sigmas_t = extract_into_tensor(sigmas, t, x_t.shape)\n",
    "    alphas_t = extract_into_tensor(alphas, t, x_t.shape)\n",
    "\n",
    "    # Выставляем крайние значения alpha и sigma, чтобы выполнялись граничные условия\n",
    "    alphas_s[s == 0] = 1.0\n",
    "    sigmas_s[s == 0] = 0.0\n",
    "\n",
    "    alphas_t[t == 0] = 1.0\n",
    "    sigmas_t[t == 0] = 0.0\n",
    "\n",
    "    x_0 = <YOUR CODE HERE> # x0 оценка на шаге t\n",
    "    x_s = <YOUR CODE HERE> # Переход на шаг s\n",
    "    return x_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4013fc3",
   "metadata": {
    "id": "c4013fc3"
   },
   "source": [
    "####  Реализация процесса зашумления (q sample)\n",
    "\n",
    "Аналогично, нам нужен процесс зашумления $q(\\mathbf{x}_t | \\mathbf{x}_0)= {N}(\\mathbf{x}_t | \\alpha_t \\mathbf{x}_0, \\sigma^2_t I)$\n",
    "\n",
    "$\\mathbf{x}_t = \\alpha_t \\mathbf{x}_0 + \\sigma_t \\epsilon$, где $\\epsilon{\\sim} {N}(0, I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb169c",
   "metadata": {
    "id": "9fdb169c"
   },
   "outputs": [],
   "source": [
    "def q_sample(x, t, scheduler, noise=None):\n",
    "    alphas = <YOUR CODE HERE>\n",
    "    sigmas = <YOUR CODE HERE>\n",
    "\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "    sigmas_t = extract_into_tensor(sigmas, t, x.shape)\n",
    "    alphas_t = extract_into_tensor(alphas, t, x.shape)\n",
    "\n",
    "    x_t = <YOUR CODE HERE>\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5893c",
   "metadata": {
    "id": "21c5893c"
   },
   "source": [
    "# Consistency Training\n",
    "\n",
    "Обучение консистенси моделей без учителя называется Consistency Training (CT).\n",
    "В таком случае CM можно рассматривать как отдельный вид генеративных моделей.\n",
    "Давайте начнем именно с этого подхода и обучим нашу первую консистенси модель на базе SD1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5743b1",
   "metadata": {
    "id": "6d5743b1"
   },
   "source": [
    "## Задание №3 |  Теория (0.5 балл)\n",
    "\n",
    "В консиcтенси дистилляции модель учителя используется для получения второй точки на траектории ODE.\n",
    "Можем ли мы попробовать оценить соседнюю точку аналитически?\n",
    "\n",
    "Вам предлагается вывести это самим, используя формулу DDIM шага выше и вспомнив, как мы оцениваем скор функции в denoising score matching-e:\n",
    "\n",
    "$\\epsilon_\\theta(x_t, t) = - \\sigma_t s_\\theta(x_t, t)$\n",
    "\n",
    "$s_\\theta(x_t, t) \\approx \\nabla_{x_t} \\log q(x_t) = \\mathop{\\mathbb{E}}_{\\mathbf{x}\\sim p_{data}}\\left [ \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t | \\mathbf{x}) \\vert \\mathbf{x}_t \\right ] \\approx \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t \\vert \\mathbf{x})$\n",
    "\n",
    "---\n",
    "\n",
    "< YOUR DERIVATION HERE >\n",
    "    \n",
    "$x_s = ?$\n",
    "\n",
    "---\n",
    "\n",
    "Если возникнут трудности, можно обратиться к оригинальной [статье](https://arxiv.org/pdf/2303.01469).\n",
    "\n",
    "**Важно: требуется предоставить вывод, а не только финальный результат.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a321f6b",
   "metadata": {
    "id": "6a321f6b"
   },
   "source": [
    "Теперь реализуем то, что у вас получилось в функции ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58cba2",
   "metadata": {
    "id": "ff58cba2"
   },
   "outputs": [],
   "source": [
    "def get_xs_from_xt_naive(\n",
    "    x_0, x_t, t, s, # Не все эти аргументы могут быть вам нужны.\n",
    "    scheduler,\n",
    "    noise=None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Получение точки x_s в CT режиме, т.е., аналитически.\n",
    "    \"\"\"\n",
    "    x_s = <YOUR CODE HERE>\n",
    "    return x_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967d0f5",
   "metadata": {
    "id": "a967d0f5"
   },
   "source": [
    "## Задание №4 (2.5 балла)\n",
    "\n",
    "Подготовить процедуру обучения консистенси моделей. Для этого нам потребуется:\n",
    "\n",
    "* Подготовить функцию для подсчета лосса\n",
    "* Сделать эффективный цикл обучения\n",
    "\n",
    "**Замечание:** функции ниже будут активно использоваться во всей дальнейшей домашке. \n",
    "\n",
    "Ниже предстален шаблон функции, которая считает лосс для консистенси моделей. \n",
    "Вам нужно заполнить пропуски, чтобы получилась корректная функция. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675acdd5",
   "metadata": {
    "id": "675acdd5"
   },
   "outputs": [],
   "source": [
    "def cm_loss_template(\n",
    "    latents, prompt_embeds, # батч латентов и текстовых эмбедов\n",
    "    unet, scheduler,\n",
    "\n",
    "    # Функции, которые будем постепенно менять из задания к заданию\n",
    "    loss_fn: callable,\n",
    "    get_boundary_timesteps: callable,\n",
    "    get_xs_from_xt: callable,\n",
    "\n",
    "    num_timesteps=1000,\n",
    "    step_size=20, # Указываем с каким интервалом берем шаги s и t.\n",
    "):\n",
    "    # Сэмплируем случайные шаги t для каждого элемента батча t ~ U[step_size-1, 999]\n",
    "    assert num_timesteps == 1000\n",
    "    num_intervals = num_timesteps // step_size\n",
    "\n",
    "    index = torch.randint(1, num_intervals, (len(latents),), device=latents.device).long() # [1, num_intervals]\n",
    "    t = <YOUR CODE HERE>\n",
    "    s = <YOUR CODE HERE>\n",
    "    boundary_timesteps = get_boundary_timesteps(\n",
    "        s, num_timesteps=num_timesteps\n",
    "    )\n",
    "\n",
    "    # Сэмплируем x_t\n",
    "    noise = torch.randn_like(latents)\n",
    "    x_t = <YOUR CODE HERE>\n",
    "\n",
    "    # Предсказание \"онлайн\" моделью\n",
    "    # with <YOUR CODE HERE>: # для реализации mixed-precision обучения\n",
    "    noise_pred = <YOUR CODE HERE>\n",
    "\n",
    "    # Получаем оценку в граничной точке для x_t \n",
    "    boundary_pred = <YOUR CODE HERE>\n",
    "\n",
    "    # Получаем сэмпл x_s из x_t\n",
    "    x_s = get_xs_from_xt(\n",
    "        latents, x_t, t, s,\n",
    "        scheduler,\n",
    "        prompt_embeds=prompt_embeds,\n",
    "        noise=noise,\n",
    "    )\n",
    "\n",
    "    # Предсказание \"таргет\" моделью.\n",
    "    with torch.no_grad(), torch.amp.autocast(\"cuda\", torch.float16):\n",
    "        target_noise_pred = <YOUR CODE HERE>\n",
    "\n",
    "    # Получаем оценку в граничной точке для x_s\n",
    "    boundary_target = <YOUR CODE HERE>\n",
    "\n",
    "    loss = loss_fn(boundary_pred, boundary_target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe34d9",
   "metadata": {},
   "source": [
    "Теперь зададим параметры этой функции для Consistency Training случая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c0c60d",
   "metadata": {
    "id": "63c0c60d"
   },
   "outputs": [],
   "source": [
    "def get_zero_boundary_timesteps(t, **kwargs):\n",
    "    \"\"\"\n",
    "    Определяем шаги где будут срабатывать граничные условия.\n",
    "    Для классических СM это t=0.\n",
    "    \"\"\"\n",
    "    return torch.zeros_like(t)\n",
    "\n",
    "\n",
    "ct_loss = functools.partial(\n",
    "    cm_loss_template,\n",
    "\n",
    "    loss_fn=mse_loss,\n",
    "    get_boundary_timesteps=get_zero_boundary_timesteps,\n",
    "    get_xs_from_xt=get_xs_from_xt_naive\n",
    ")\n",
    "assert cm_unet.active_adapter == 'ct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c526cc4",
   "metadata": {
    "id": "5c526cc4"
   },
   "source": [
    "### Обучающий цикл\n",
    "\n",
    "Вам дан код обучения модель в полной точности (FP32) c батчом 8.\n",
    "К сожалению, на Tesla T4 мы скорее всего не влезем по памяти.\n",
    "Поэтому в ячейке ниже вам нужно модифицировать цикл, чтобы он работал в mixed precision FP16 и добавить gradient accumulation.\n",
    "\n",
    "Про реализацию mixed-precision в pytorch можно перейти по ссылке: [Mixed-precision обучение](https://pytorch.org/docs/stable/notes/amp_examples.html#typical-mixed-precision-training)\n",
    "\n",
    "**Обратите внимание**: вам еще нужно добавить одну строчку кода в *cm_loss_template* в соответствующем плейсхолдере.\n",
    "\n",
    "### Эффективное обучение\n",
    "\n",
    "**По аналогии с Частью 2 в ДЗ №1**\n",
    "\n",
    "Данное задание рассчитано на успешное выполнение на colab с бесплатной Tesla T4 c 15GB VRAM.\n",
    "Однако учить даже относительно небольшие T2I модели масштаба SD1.5 на коллабе в лоб проблематично.\n",
    "\n",
    "Для этого полезно применить ряд инженерных техник, чтобы уместиться в данный бюджет и учиться за разумное время.\n",
    "\n",
    "**Список техник**\n",
    "\n",
    "1) Включить **gradient checkpointing** для обучемой модели\n",
    "2) Добавить **LoRA** (Low Rank Adapters) адаптеры, чтобы учить не все веса, а только 10% добавочных весов\n",
    "3) Использовать **gradient accumulation**, чтобы делать итерацию обучения по бОльшему батчу, чем влезает по памяти\n",
    "4) Добавить **mixed precision** FP16/FP32 обучение модели для скорости. Обычно еще и память экономится, но в случае LoRA обучения + gradient checkpointing на память сильно влиять не должно, но зато станет быстрее.\n",
    "5) **Мульти-GPU** обучение - распределение вычислений по нескольким GPU.  \n",
    "\n",
    "**Что имеем на данный момент?**\n",
    "\n",
    "1-2) Уже сделано выше\n",
    "\n",
    "3 ) **В ДЗ №1 c памятью проблем не должно было быть, а тут батч 8 уже может не влезать. Так что вероятно придется реализовать аккумуляцию.**\n",
    "\n",
    "4 ) **Крайне полезно добавить в контексте скорости обучения. Точно также как в ДЗ №1** \n",
    "\n",
    "5 ) Недоступно, так как работаем на одной карточке\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QoidXNBgZgkY",
   "metadata": {
    "id": "QoidXNBgZgkY"
   },
   "outputs": [],
   "source": [
    "def train_loop(model, pipe, train_dataloader, optimizer, loss_fn, num_grad_accum=1):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        latents, prompt_embeds = prepare_batch(batch, pipe)\n",
    "\n",
    "        loss = loss_fn(latents, prompt_embeds, model, pipe.scheduler)\n",
    "\n",
    "        # Обновляем параметры\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.detach().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a5f675",
   "metadata": {
    "id": "79a5f675"
   },
   "outputs": [],
   "source": [
    "num_grad_accum = 2 # обновляем параметры каждые 2 шага\n",
    "\n",
    "train_loop(cm_unet, pipe, train_dataloader, optimizer, ct_loss, num_grad_accum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8808dc13",
   "metadata": {
    "id": "8808dc13"
   },
   "source": [
    "## Задание 5 (1.25 балла)\n",
    "\n",
    "### Генерация с помощью обученной консистенси модели\n",
    "\n",
    "Настало время погенерировать картинки с помощью нашей модели.\n",
    "Напомним, что мы не можем для консистенси моделей использовать DDIM и другие классические солверы для диффузии.\n",
    "Нам нужен специальный сэмплер для CM, который схематично изображен на картинке ниже:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/cd-sampling-idea.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Чуть более формально:\n",
    "\n",
    "$x_{t_n} \\sim {N}(0, I)$\n",
    "\n",
    "$for\\ t_i \\in [t_n, ..., t_1]:$\n",
    "\n",
    "* $\\epsilon \\leftarrow unet(x_{t_i})$\n",
    "\n",
    "* $x_0 \\leftarrow DDIM(\\epsilon, x_{t_i}, t_i, 0)$\n",
    "\n",
    "* $x_{t_{i-1}} \\leftarrow q(x_{t_{i-1}} | x_0)$\n",
    "\n",
    "\n",
    "**Classifier-free guidance (CFG)**\n",
    "\n",
    "Также вам надо реализовать поддержку CFG в CM сэмплирование. Вспомним формулу:\n",
    "\n",
    "$\\epsilon_w = {\\color{blue}{\\epsilon_{uncond}}} + w \\cdot (\\epsilon_{cond} - \\epsilon_{uncond})$, где $w \\geq 1$\n",
    "\n",
    "**Обратим внимание**, что режим \"без гайденса\" соотвествует $w = 1$, что немного контринтуитивно, но в большинстве реализаций будет встречаться именно такой вид этой формулы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c47c2",
   "metadata": {
    "id": "6f1c47c2"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def consistency_sampling(\n",
    "    pipe,\n",
    "    prompt,\n",
    "    num_inference_steps=4,\n",
    "    generator=None,\n",
    "    num_images_per_prompt=4,\n",
    "    guidance_scale=1\n",
    "):\n",
    "    if prompt is not None and isinstance(prompt, str):\n",
    "        batch_size = 1\n",
    "    elif prompt is not None and isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "\n",
    "    device = pipe._execution_device\n",
    "\n",
    "    # Извлекаем эмбеды из текстовых промптов. Реализуйте вызов pipe.encode_prompt\n",
    "    do_classifier_free_guidance = guidance_scale > 0\n",
    "    prompt_embeds, null_prompt_embeds = <YOUR CODE HERE>\n",
    "    assert prompt_embeds.dtype == null_prompt_embeds.dtype == torch.float16\n",
    "\n",
    "    # Настраиваем параметры scheduler-a\n",
    "    assert pipe.scheduler.config['timestep_spacing'] == 'trailing'\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    # Создаем батч латентов из N(0,I)\n",
    "    latents = <YOUR CODE HERE>\n",
    "\n",
    "    for i, t in enumerate(tqdm(pipe.scheduler.timesteps)):\n",
    "        t = torch.tensor([t] * len(latents)).to(device)\n",
    "        zero_t = torch.tensor([0] * len(latents)).to(device)\n",
    "\n",
    "        cond_noise_pred = <YOUR CODE HERE>\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            uncond_noise_pred = <YOUR CODE HERE>\n",
    "            noise_pred = <YOUR CODE HERE>\n",
    "        else:\n",
    "            noise_pred = cond_noise_pred\n",
    "\n",
    "        # Получаем x_0 оценку из x_t\n",
    "        x_0 = <YOUR CODE HERE>\n",
    "\n",
    "        if i + 1 < num_inference_steps:\n",
    "            # Переход на следующий шаг\n",
    "            s = pipe.scheduler.timesteps[i+1]\n",
    "            s = torch.tensor([s] * len(latents)).to(device)\n",
    "\n",
    "            latents = <YOUR CODE HERE>\n",
    "        else:\n",
    "            # Последний шаг\n",
    "            latents = x_0\n",
    "\n",
    "        latents = latents.half()\n",
    "\n",
    "    # Декодируем латенты в пиксели. Не забудьте про pipe.vae.config.scaling_factor \n",
    "    image = <YOUR CODE HERE>\n",
    "    do_denormalize = [True] * image.shape[0]\n",
    "    image = pipe.image_processor.postprocess(image, output_type=\"pil\", do_denormalize=do_denormalize)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57927112",
   "metadata": {
    "id": "57927112"
   },
   "source": [
    "Попробуем сгененировать что-то нашей моделью. Можно поиграться с разными сидами и гайденс скейлами.\n",
    "\n",
    "Референс, что примерно должно получиться на этом этапе для guidance_scale=2. Как видите, картинки стали почетче, но пока все еще так себе.\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/reference-ct.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2dc5a8",
   "metadata": {
    "id": "cf2dc5a8"
   },
   "outputs": [],
   "source": [
    "pipe.unet = cm_unet.eval().to(torch.float16)\n",
    "assert cm_unet.active_adapter == 'ct'\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(1)\n",
    "guidance_scale = 3\n",
    "\n",
    "# Заменяем генерацию пайплайном на наше сэмплирование.\n",
    "images = <YOUR CODE HERE>\n",
    "\n",
    "visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11670976",
   "metadata": {
    "id": "11670976"
   },
   "source": [
    "# Consistency Distillation\n",
    "\n",
    "## Задание №6 (1.25 балла)\n",
    "\n",
    "\n",
    "Теперь давайте попробуем перейти к постановке дистилляции, где шаг из $x_t$ в $x_s$ будет делаться не аналитически, а c помощью модели учителя.  \n",
    "\n",
    "$\\mathbf{x}_t = q(\\mathbf{x}_t | \\mathbf{x}_0)$\n",
    "\n",
    "$\\mathbf{x}_s = DDIM(\\epsilon_\\theta(\\mathbf{x}_t, t), \\mathbf{x}_t, t, s)$\n",
    "\n",
    "**Замечание:**\n",
    "В text-to-image генерации *classifier-free guidance (CFG)* играет очень важную роль для получения хорошего качества с помощью диффузии.\n",
    "CFG меняет траектории ODE и раз нам он важен, то давайте и дистиллировать траектории с CFG.\n",
    "\n",
    "Поэтому для получения точки $\\mathbf{x}_{s}$ мы будем использовать шаг учителя с CFG. Это важное отличие от CT сеттинга - там мы не можем моделировать гайденс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98399cfb",
   "metadata": {
    "id": "98399cfb"
   },
   "outputs": [],
   "source": [
    "unet = unet.to(torch.float32)\n",
    "unet.train()\n",
    "assert unet.dtype == torch.float32\n",
    "\n",
    "# Добавляем новые LoRA адаптеры для CD модели\n",
    "cm_unet.add_adapter(\"cd\", lora_config)\n",
    "cm_unet.set_adapter(\"cd\")\n",
    "\n",
    "# Пересоздаем оптимизатор\n",
    "optimizer = torch.optim.AdamW(cm_unet.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eda898",
   "metadata": {
    "id": "67eda898"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_xs_from_xt_with_teacher(\n",
    "    x_0, x_t, t, s, # Не все эти аргументы могут быть вам нужны\n",
    "    scheduler,\n",
    "    prompt_embeds,\n",
    "    teacher_unet,\n",
    "    guidance_scale,\n",
    "    **kwargs\n",
    "):\n",
    "    # Делаем предсказание учителем в кондишион случае: подаем эмбеды текста\n",
    "    cond_noise_pred = <YOUR CODE HERE>\n",
    "\n",
    "    # Для CFG нам нужно делать предсказания в unconditional случае.\n",
    "    # Для T2I моделей, мы будем это моделировать предсказаниями для пустого промпта \"\"\n",
    "    # Извлечем эмбеды из пустого промпта и размножить их до размера батча\n",
    "    uncond_prompt_embeds = <YOUR CODE HERE>\n",
    "\n",
    "    # Затем прогоняем модель для пустых промптов\n",
    "    uncond_noise_pred = <YOUR CODE HERE>\n",
    "\n",
    "    # Применяем CFG формулу и получаем итоговый предикт учителя\n",
    "    noise_pred = <YOUR CODE HERE>\n",
    "\n",
    "    # Получаем x_s из x_t\n",
    "    x_s = <YOUR CODE HERE>\n",
    "    return x_s\n",
    "\n",
    "\n",
    "# Сразу зададим внутрь модель учителя и guidance_scale\n",
    "get_xs_from_xt_with_teacher = functools.partial(\n",
    "    get_xs_from_xt_with_teacher,\n",
    "    teacher_unet=teacher_unet,\n",
    "    guidance_scale=7.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f39be",
   "metadata": {
    "id": "765f39be"
   },
   "source": [
    "Еще, как показано в работе [Improved Techniques for Training Consistency Models](https://arxiv.org/pdf/2310.14189).\n",
    "L2 лосс не самый оптимальный выбор для консистенси моделей.\n",
    "Давайте в CD обучении также заменим MSE лосс на pseudo-huber лосс из статьи.\n",
    "\n",
    "**Обратите внимание:** в публичных реализациях и в статье лоссы могут отличаться. Реализация из статьи ок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a5bac",
   "metadata": {
    "id": "878a5bac"
   },
   "outputs": [],
   "source": [
    "def pseudo_huber_loss(\n",
    "    x: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    c=0.001\n",
    "):\n",
    "    loss = <YOUR CODE HERE>\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12855c88",
   "metadata": {
    "id": "12855c88"
   },
   "outputs": [],
   "source": [
    "cd_loss = functools.partial(\n",
    "    cm_loss_template,\n",
    "\n",
    "    loss_fn=pseudo_huber_loss,\n",
    "    get_boundary_timesteps=get_zero_boundary_timesteps,\n",
    "    get_xs_from_xt=get_xs_from_xt_with_teacher\n",
    ")\n",
    "\n",
    "assert cm_unet.active_adapter == 'cd'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db78ad",
   "metadata": {
    "id": "79db78ad"
   },
   "source": [
    "**Теперь обучим модель в CD режиме**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3035cf7",
   "metadata": {
    "id": "a3035cf7"
   },
   "outputs": [],
   "source": [
    "num_grad_accum = 2 # обновляем параметры каждые 2 шага\n",
    "\n",
    "train_loop(cm_unet, pipe, train_dataloader, optimizer, cd_loss, num_grad_accum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971622aa",
   "metadata": {
    "id": "971622aa"
   },
   "source": [
    "### Снова сэмплируем\n",
    "\n",
    "Обратим внимание, что тут мы сэмпилруем без гайденса, потому что мы его уже частично прокинули в модель, когда делали шаг учителя с CFG.\n",
    "\n",
    "Снова для референса приводим картинки на этом этапе:\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/reference-cd.png)\n",
    "\n",
    "**Ваши картинки не обязаны совпадать: у вас могут быть немного менее/более качественные. Небольшая разница по качеству на оценку не влиет.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e13eb6",
   "metadata": {
    "id": "46e13eb6"
   },
   "outputs": [],
   "source": [
    "# Подставляем нашу новую обученную модель в пайплайн\n",
    "pipe.unet = cm_unet.eval().to(torch.float16)\n",
    "assert cm_unet.active_adapter == 'cd'\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(0)\n",
    "guidance_scale = 0\n",
    "\n",
    "images = <YOUR CODE HERE>\n",
    "\n",
    "visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be7c1e",
   "metadata": {
    "id": "30be7c1e"
   },
   "source": [
    "#### Давайте посмотрим на картинки для других промптов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae76af",
   "metadata": {
    "id": "4bae76af"
   },
   "outputs": [],
   "source": [
    "validation_prompts = [\n",
    "    \"A sad puppy with large eyes\",\n",
    "    \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\",\n",
    "    \"A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece\",\n",
    "    \"A girl with pale blue hair and a cami tank top\",\n",
    "    \"A lighthouse in a giant wave, origami style\",\n",
    "    \"belle epoque, christmas, red house in the forest, photo realistic, 8k\",\n",
    "    \"A small cactus with a happy face in the Sahara desert\",\n",
    "    \"Green commercial building with refrigerator and refrigeration units outside\",\n",
    "]\n",
    "\n",
    "for prompt in validation_prompts:\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(0)\n",
    "\n",
    "    images = <YOUR CODE HERE>\n",
    "\n",
    "    visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294acc5",
   "metadata": {
    "id": "9294acc5"
   },
   "source": [
    "# Multi-boundary Сonsistency Distillation\n",
    "<div>\n",
    "<img src=https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/multi-cd-idea.jpg width=600>\n",
    "<div>\n",
    "    \n",
    "   В конце мы рассмотрим недавнюю модификацию CD, *Multi-boundary CD*, где интегрируем не всю траекторию сразу и потом сэмплируем с возвращением назад, а разбиваем траектории на $K$ отрезков и применяет CD внутри каждого отрезка независимо. Например, на картинке выше у нас два отрезка: зеленым и красным выделены две граничные точки. \n",
    "Для классического CD, рассмотренного ранее, у нас только одна граничная точка в $t = 0$ \n",
    "    \n",
    "**Обратим внимание**, что сэмплирование становится детерминистичным и можно снова использовать DDIM солвер, где число шагов равно числу интервалов $K$, на которые мы разбили траектории во время обучения.\n",
    "\n",
    "Этот метод гораздо лучше работает чем обычный CD, потому что решать задачу CD на отрезках, а не на всей траектории, гораздо проще. В текущем задании мы разобьем траекторию на $K=4$ отрезка.\n",
    "\n",
    "Подробнее почитать можно в этой [статье](https://arxiv.org/pdf/2403.06807)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f05839",
   "metadata": {
    "id": "95f05839"
   },
   "source": [
    "## Задание №7 (1 балл)\n",
    "\n",
    "Ниже реализуйте функцию, которая для $K=4$ отрезков будет сопоставлять таймстепам соответствующие граничные точки.\n",
    "\n",
    "Например, для $K=2$ отрезков граничные точки будут: [0, 499]\n",
    "\n",
    "$0 \\leq t < 499$ -> граничная точка - $0$\n",
    "\n",
    "$499 \\leq t < 999$ -> граничная точка - $499$\n",
    "\n",
    "**Замечание:** помним, что интервал между $t$ и $s$ - 20 шагов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2dbb2",
   "metadata": {
    "id": "c7d2dbb2"
   },
   "outputs": [],
   "source": [
    "def get_multi_boundary_timesteps(\n",
    "    timesteps,\n",
    "    num_boundaries=4,\n",
    "    num_timesteps=1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Для батча таймстепов определяем соответствующие граничные точки.\n",
    "    params:\n",
    "        timesteps: torch.Tensor(batch_size, device='cuda')\n",
    "    returns:\n",
    "        boundary_timesteps: torch.Tensor(batch_size, device='cuda')\n",
    "    \"\"\"\n",
    "    # Здесь важно повыводить timesteps и boundary_timesteps перед обучением, \n",
    "    # чтобы не перелетать граничные точки и при этом иногда попадать в них.\n",
    "  \n",
    "    <YOUR CODE HERE>\n",
    "    return boundary_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1c0eb",
   "metadata": {
    "id": "c4c1c0eb"
   },
   "outputs": [],
   "source": [
    "multi_cd_loss = functools.partial(\n",
    "    cm_loss_template,\n",
    "\n",
    "    loss_fn=pseudo_huber_loss,\n",
    "    get_boundary_timesteps=get_multi_boundary_timesteps,\n",
    "    get_xs_from_xt=get_xs_from_xt_with_teacher\n",
    ")\n",
    "assert cm_unet.active_adapter == 'multi-cd'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86175a",
   "metadata": {
    "id": "3d86175a"
   },
   "source": [
    "**Теперь обучим  Multi-boundary CD модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded4086",
   "metadata": {
    "id": "aded4086"
   },
   "outputs": [],
   "source": [
    "num_grad_accum = 2 # обновляем параметры каждые 2 шага\n",
    "\n",
    "train_loop(cm_unet, pipe, train_dataloader, optimizer, multi_cd_loss, num_grad_accum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28f872",
   "metadata": {
    "id": "2e28f872"
   },
   "source": [
    "### И в последний раз сэмплируем\n",
    "\n",
    "**Важно: Стохастический сэмплинг теперь не годится, но теперь у нас появляется возможноcть сэмплировать детерминистично с помощью оригинального солвера DDIM за 4 шага.**\n",
    "\n",
    "Ниже прикрепляем референс и напомним, что у вас картинки могут отличаться и быть чуть хуже/лучше.\n",
    "![img](https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/reference-multi-cd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa9bcc",
   "metadata": {
    "id": "90aa9bcc"
   },
   "outputs": [],
   "source": [
    "pipe.unet = cm_unet.eval().to(torch.float16)\n",
    "assert cm_unet.active_adapter == 'multi-cd'\n",
    "\n",
    "guidance_scale = 1\n",
    "\n",
    "for prompt in validation_prompts:\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(1)\n",
    "\n",
    "    images = <YOUR CODE HERE>\n",
    "\n",
    "    visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508112b9",
   "metadata": {
    "id": "508112b9"
   },
   "source": [
    "**На этом все! Ура!**\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/yoda.jpg\" width=400>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721fcfe5",
   "metadata": {
    "id": "dh7XtMVcUVDm"
   },
   "source": [
    "### P.S. Некоторые примеры плохих генераций, которые могут возникать при выполнении задания\n",
    "\n",
    "#### Неправильный сэмплинг\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/fail_case_1.jpg)\n",
    "![img](https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/fail_case_2.jpg)\n",
    "\n",
    "#### Ошибки в обучении\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/fail_case_3.jpg)\n",
    "![img](https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/fail_case_4.jpg)\n",
    "\n",
    "#### Необученная модель\n",
    "\n",
    "![img](https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/fail_case_5.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf27323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
